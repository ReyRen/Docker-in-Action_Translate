​	软件的使用是复杂的。在安装软件之前，你必须考虑你使用的操作系统是什么，其中已安装的软件以及该软件所需要的依赖有些什么。并且现在各式各样的软件的安装方式千变万化。所以软件的安装是不一致并且过于复杂的，如果你想保持多个机器的软件协同，那么难度会更大。

​	包管理工具例如APT、Homebrew、Yum，以及npm尝试进行软件的管理，但是几乎没有提供层级的隔离。大部分的计算机是有大于1个应用程序已安装的。并且大部分的应用对其他的应用有着或多或少的依赖。但是如果你想依赖的程序出错了怎么办？那将是灾难性的麻烦：

- 如果相互依赖的两个应用程序，其中一个需要升级，但是另外一个不需要会发生什么？
- 但你删除一个应用的时候，它真的就移除了吗？
- 你能够移除旧的依赖吗？

​	真相就是你用的软件越多，那么复杂性越高。即使是企业级的服务器软件也是有依赖部署问题的。这些项目通常会部署到具有成千上万个文件和其他程序的机器上。这其中的每一项都可能成为造成冲突和安全的因素。

​	虽然这些问题可以通过细致的统计、管理以及逻辑整理来解决，但是这是谁也不愿意多花时间浪费在这上面的事情，你更希望时间就仅仅用在安装、更新以及卸载上。那些构建了Docker的人确实付出了很多努力，以至于可以是你轻而易举的解决问题。

### 得到组织

​	没有Docker，计算机就像是个乱七八糟的抽屉。不同的应用有着各种各样的依赖，有的依赖于系统本身的一些库，有的依赖于一些语言的标准库。一些取绝于另外一些应用，比如Java程序依赖于JVM，或者一个web应用依赖于数据库。正在运行的程序有时需要对一些稀缺资源（网络或文件等）进行独占访问。

### 提高了便携性

​	软件的另外一个问题就是对操作系统的依赖性，在操作系统之间移动可能会对软件的安装造成问题。即使由一些对linux和macos兼容的软件，但是对windows下就麻烦了。

​	到目前为止，Docker原生运行与Linux上并且携带有macOS和Windows的虚拟机。这种在linux下的融合意味着在Docker容器中的软件只需要针对一组一致的依赖编写一次。但是之前不是说Docker要优于虚拟机吗？是的，这两种技术其实是相辅相成。在一个虚拟机中运行单一的程序过于浪费。尤其是当你在一台物理机上运行多个虚拟机的时候。在Macos和Windows上，docker通过运行一个小的虚拟机从而能够运行像linux中一样的容器。通过这种方式，过于庞大的虚拟机得到了解决，并且通过大量的容器对应用的规模性也得到了解决。

​	没有Docker和虚拟机，便携的软件能力只能通过在这软件层自己写一些东西，比如为了做到Java程序的可移植性，开发出了JVM，但是如果我们想要别的便携性语言开发一个web程序之类的，很有可能没有这些类似于JVM一样的环境，经而使得过程变得复杂，你需要根据不同的操作系统来进行定制开发。除此之外，语言的解释器、编译器和库都是问题。Docker提高的便携性可以做到不管操作系统是什么，不管任何语言是什么。

### 保护你的电脑

​	到目前为止，我们讨论的是软件安装部署的问题。其实程序对计算机的一些不好的行为是可以造成挺严重的后果的：

- 一个程序可能已经被黑客篡改了
- 再牛逼的开发者都可能写出有伤害性的bug
- 程序可能通过其输入处理中的错误意外的执行力攻击着的命令

​	因为运行程序是在整个计算机上运行，所以还是很危险的。就像物理的牢房一样，容器中的任何东西也是只能访问其中的东西。当然存在此规则外的例外情况，但是这种情况也是用户指定的。容器限制了一个程序对其他正在运行的程序、它可以访问的数据以及系统资源的影响范围。

​	这对你或企业意味着，与运行特定应用程序相关的任何安全威胁的范围都仅限于应用程序本身的范围。 创建强大的应用程序容器很复杂，并且是所有深度防御策略的关键组成部分。